#!/usr/bin/env node

const process = require("node:process")
const fs = require("fs/promises")
const http = require("http");
const {URL} = require("url");

const print = s => process.stdout.write(s)
const msg   = s => process.stderr.write(s)

function printHelp() {
    msg("Usage: gen-ollama [options] prompt.\n\n")
    msg("Options: \n")
    msg("    --out    FILE          Output LLM response lines to FILE (env: OUT_FILE)\n")
    msg("    --model  MODEL_NAME    Use model MODEL_NAME (env: LLM_MODEL)\n")
    msg("    --server SERVER_URL    Use SERVER_URL as endpoint base name (env: SERVER_URL)\n")
    msg("    --memory MEMORY_FILE   Use the given file to save the chat for continue (env: LLM_MEMORY_FILE)\n")
    msg("\n")
    msg("Note: '-' in arg list will stop param reading and take prompt from STDIN\n")
    msg("\n")
    msg("Env:\n")
    msg("    LLM_SYSTEM_PROMPT_FILE    Provide a text file as system prompt.\n")
    msg("    LLM_KEEP_ALIVE            How long for keep LLM model in memory, default is '5m'.\n")
    msg("    LLM_SYSTEM_PROMPT_ROLE    Role name of system prompt, default is 'system'.\n")
    msg("    LLM_CTX_WINDOW_SIZE       Context window size, default is 2048.\n")
    msg("\n")
}
function printHelpE(n) {
    printHelp(); process.exit(n);
}
//
// Get Settings
//
function processArgs(args) {
    if(!args || args.length <= 2) return printHelpE(1);

    const result = {}

    const lastArg = args[args.length - 1]
    if(lastArg.substring(0, 2) === "--") return printHelpE(2);

    if(lastArg !== '-') result.prompt = lastArg;

    const options = args.slice(2, args.length - 1)
    if(options.length <= 0) return result

    let op, i;
    function inbound(size) {
        i += size
        if(i < options.length) return i;
        (msg("Require value for option: " + op + "\n\n\n"),printHelpE(3))
    }
    for ((i = 0, op = options[i]); i < options.length; i++) {
        switch (op) {
            case "--out":
                result.out = options[inbound(1)];
                break;
            case "--model" :
                result.model = options[inbound(1)];
                break;
            case "--server" :
                result.serverUrl = options[inbound(1)];
                break;
            case "--memory":
                result.memoryFile = options[inbound(1)];
                break;
            case "-":
                result.prompt = null;
                return result;
            default:
                printHelp();
                process.exit(4);
                break;
        }
    }

    return result
}

const buildConfig = () => {
    const result = {}

    result.serverUrl      = process.env["SERVER_URL"] || "http://10.0.20.1:11434"
    result.model          = process.env["LLM_MODEL"] 
    result.out            = process.env["OUT_FILE"]
    result.systemPrompt   = process.env["LLM_SYSTEM_PROMPT_FILE"]
    result.modelKeepAlive = process.env["LLM_KEEP_ALIVE"]
    result.memoryFile     = process.env["LLM_CHAT_MEMORY"]
    result.systemRole     = process.env["LLM_SYSTEM_PROMPT_ROLE"] || "system"
    result.llmCtxSize     = process.env["LLM_CTX_WINDOW_SIZE"]

    return { ...result, ...processArgs(process.argv) }
}

//
// Post Hooks
//
const [postHooks, postHook, runPostHooks] = [
    [], (a) => { postHooks.push(a) }, () => Promise.all(postHooks.map((a) => a()))
]

//
// Json Streaming Helper
//

function fetchJsonStream({url, body, headers, onData}) {
    const {hostname, port, pathname} = URL.parse(url)
    let buffer = ''
    const onEachData = (chunk, reject) => {
        buffer += chunk

        while (true) {
            const newLineIndex = buffer.indexOf('\n');
            if (newLineIndex === -1) break;

            const line = buffer.slice(0, newLineIndex);
            buffer = buffer.slice(newLineIndex + 1);

            if (!line.trim()) continue;

            try {
                const obj = JSON.parse(line);
                if(obj === null) continue;
                onData(obj);
            } catch (error) {
                reject(new Error(`Invalid JSON in line: ${error.message}`))
            }
        }
    }

    const postBody = JSON.stringify(body) + '\n'

    const options = {
        hostname, port, path: pathname,
        method: 'POST',
        headers: {
            'Content-Type': 'application/json'
        }
    }

    return new Promise((resolve, reject) => {

        const request = http.request(options, resp => {
            if (resp.statusCode !== 200) {
                let errMsg = `[[ HTTP STATUS ${resp.statusCode}, RESPONSE `
                new Promise((r) => {
                    resp.on('data', c => errMsg+= c); resp.on('end', r);
                }).then(() => reject(`${errMsg} ]]`))
                return
            }

            resp.on('data', chunk => onEachData(chunk, reject));
            resp.on('end', () => {
                if (buffer) reject(new Error("Incomplete JSON received"));
                else resolve()
            });
        })
        request.on('error', reject)

        request.write(postBody, (error) => {
            if (error) reject(error);
        })
        request.end()
    })
}

async function createMemory(config) {
    const { memoryFile } = config
    if(!memoryFile) return null

    let memory = {}
    try {
        msg("[[ Chat memory: '" + memoryFile + "' ]]\n")
        await fs.access(memoryFile)
        const content = await fs.readFile(memoryFile, 'utf8')
        const jsonContent = JSON.parse(content)
        if(typeof jsonContent === 'object' && jsonContent !== null) memory = jsonContent
    } catch (e) {
        // Do nothing
    }
    if(!memory.messages) memory.messages = []

    async function saveAsync() {
        // Push the assistant response
        if(this.responseBuffer) this.messages.push({ "role" : "assistant", content: this.responseBuffer })

        const memory = { messages: this.messages }
        await fs.writeFile(memoryFile, JSON.stringify(memory))
        msg("[[ Chat history saved: '" + memoryFile + "' ]]\n")
    }

    function appendAssistantOut(str) {
        if(!this.responseBuffer) this.responseBuffer = ""
        this.responseBuffer += str
    }

    function toolResponse(content) {
        this.messages.push({"role":"tool_call", content})
    }

    function userResponse(content) {
        this.messages.push({"role":"user", content})
    }

    return { messages: memory.messages || [], responseBuffer: null , appendAssistantOut, toolResponse, userResponse, saveAsync }
}

//
// Response writer
//
async function buildDataWriter(config) {
    const outFilePath = config.out
    if(!outFilePath) return () => { /* Noop */ }

    const file = await fs.open(outFilePath, 'w')
    postHook(async () => {
        await file.sync();
        await file.close();
        msg("[[ LLM Response saved to file '" + outFilePath + "' ]]\n")
    })
    return ((line) => file.write(`${line}\n`))
}

//
// Timer
//

function buildTimer() {
    function start() { this.startTime = Date.now() }
    function stop() { this.endTime = Date.now() }
    function tick() {
        const now = Date.now()
        this.tps.push(now)
        if(this.responseStart <= 0) {
            this.responseStart = now;
            return true
        }
        return false
    }
    function sum() {
        const { tps, startTime, endTime } = this
        const tpsDelta = []
        for(let i = 0; i < (tps.length - 1); i++) tpsDelta.push(tps[i + 1] - tps[i])
        const avg = Math.ceil(1000000 / (tpsDelta.reduce((p, c,) => p + c) / tpsDelta.length)) / 1000
        const elapsed = Math.ceil(endTime - startTime) / 1000
        return { avg, elapsed }
    }
    function ctxInitTime() {
        const { startTime, responseStart } = this
        return (Math.ceil(responseStart - startTime)) / 1000
    }
    return { startTime: 0, endTime: 0, responseStart: 0 ,tps: [], start, stop, tick, sum, ctxInitTime }
}

//
// Main
//

;(async () => {
    const config = buildConfig()
    const { model } = config
    if(!model) (msg("Please set the LLM model name.\n\n"),printHelpE(5));

    //
    // If has output file, open and prepare the writer
    //
    let dataWriter = await buildDataWriter(config)

    msg(`Server : ${config.serverUrl}\n`)
    msg(`Model  : ${model}\n`)
    msg(`\n`)

    const request = { model: config.model }

    // Set the keep-alive if needed
    if(config.modelKeepAlive) request.keep_alive = config.modelKeepAlive
    // Set context window size if present
    if(config.llmCtxSize) {
        const op = request.options || {}
        op.num_ctx = parseInt(config.llmCtxSize)
        request.options = op
    }

    // Create memory (if needed)
    const chatMemory = await createMemory(config)

    // Prepare message
    const messages = []
    if(config.systemPrompt) {
        const systemMessage = await fs.readFile(config.systemPrompt, { encoding: 'utf8' })
        msg(`<< System Role (${config.systemRole}) >>\n`)
        msg(systemMessage)
        msg("\n=================\n\n")
        messages.push({ role: config.systemRole, content: systemMessage })
    }
    // If has memory, restore the memory
    if(chatMemory && (chatMemory.messages.length > 0)) {
        for(const e of chatMemory.messages) messages.push(e);
        msg(`[[ Restored ${chatMemory.messages.length} chat histories. ]]\n`)
    }

    let { prompt } = config
    if (!prompt) {
        prompt = ''
        process.stdin.on('data', chunk => prompt += chunk)
        await new Promise((resolve) => process.stdin.on('end', resolve))
    }
    const userMessage = { role: "user", content: prompt }
    messages.push(userMessage)
    if(chatMemory) chatMemory.userResponse(userMessage.content)

    if (messages.length <= 0) (msg("Neither user prompt or system prompt is provided.\n"),process.exit(6))
    request.messages = messages

    msg(`<< Question : `)
    msg(`${prompt}\n`)
    msg(`>> Answer   : `)

    // Write the request body to file
    await dataWriter(JSON.stringify(request) + "\n")

    // Token speed counter
    const timer = buildTimer()
    timer.start()

    const onData = (data) => {
        if (!data) return

        dataWriter(JSON.stringify(data)).then(() => {})
            .catch((e) => msg(`[!] Fail to write line: ${e.message}\n`))

        if(timer.tick()) msg(`(Context initialize took ${timer.ctxInitTime()}s)\n`)

        const {message, done} = data;

        if (done) return msg("\n\n")
        const {content} = message
        print(content)
        chatMemory && chatMemory.appendAssistantOut(content)
    }

    // Request to server
    await fetchJsonStream({url: `${config.serverUrl}/api/chat`, body: request, onData})
    timer.stop();

    postHook(() => chatMemory.saveAsync());

    (({ avg, elapsed}) => msg(`[[ Time elapsed: ${elapsed}s, Avg: ${avg} t/s ]]\n`))(timer.sum());

    // Trigger post hooks
    await runPostHooks()
})().then(() => {}).catch(console.error);
