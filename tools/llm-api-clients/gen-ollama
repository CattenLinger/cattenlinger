#!/usr/bin/env node

const process = require("node:process")
const fs = require("fs/promises")
const http = require("http");
const {URL} = require("url");

const print = s => process.stdout.write(s)
const msg   = s => process.stderr.write(s)

function printHelp() {
    msg("Usage: gen-ollama [options] prompt.\n\n")
    msg("Options: \n")
    msg("    --out    FILE          Output LLM response lines to FILE (env: OUT_FILE)\n")
    msg("    --model  MODEL_NAME    Use model MODEL_NAME (env: LLM_MODEL)\n")
    msg("    --server SERVER_URL    Use SERVER_URL as endpoint base name (env: SERVER_URL)\n")
    msg("    --memory MEMORY_FILE   Use the given file to save the chat for continue (env: LLM_MEMORY_FILE)\n")
    msg("\n")
    msg("Note: '-' in arg list will stop param reading and take prompt from STDIN\n")
    msg("\n")
    msg("Env:\n")
    msg("    LLM_SYSTEM_PROMPT_FILE    Provide a text file as system prompt.\n")
    msg("    LLM_KEEP_ALIVE            How long for keep LLM model in memory, default is '5m'.\n")
    msg("    LLM_SYSTEM_PROMPT_ROLE    Role name of system prompt, default is 'system'.\n")
    msg("\n")
}
//
// Get Settings
//
function processArgs(args) {
    const result = {}
    if(!args || args.length <= 2) return (printHelp(),process.exit(1));

    const lastArg = args[args.length - 1]
    if(lastArg.substring(0, 2) === "--") return (printHelp(),process.exit(2));

    if(lastArg !== '-') result.prompt = lastArg;

    const options = args.slice(2, args.length - 1)
    if(options.length <= 0) return result

    let op, i;
    function inbound(size) {
        i += size
        if(i < options.length) return i;

        msg("Require value for option: " + op + "\n\n\n")
        printHelp()

        process.exit(3)
    }
    for (i = 0; i < options.length; i++) {
        op = options[i]
        switch (op) {
            case "--out":
                result.out = options[inbound(1)];
                break;
            case "--model" :
                result.model = options[inbound(1)];
                break;
            case "--server" :
                result.serverUrl = options[inbound(1)];
                break;
            case "--memory":
                result.memoryFile = options[inbound(1)];
                break;
            case "-":
                result.prompt = null;
                return result;
            default:
                printHelp();
                process.exit(4);
                break;
        }
    }

    return result
}

const buildConfig = () => {
    const result = {}

    result.serverUrl      = process.env["SERVER_URL"] || "http://10.0.20.1:11434"
    result.model          = process.env["LLM_MODEL"] 
    result.out            = process.env["OUT_FILE"]
    result.systemPrompt   = process.env["LLM_SYSTEM_PROMPT_FILE"]
    result.modelKeepAlive = process.env["LLM_KEEP_ALIVE"]
    result.memoryFile     = process.env["LLM_CHAT_MEMORY"]
    result.systemRole     = process.env["LLM_SYSTEM_PROMPT_ROLE"] || "system"

    return { ...result, ...processArgs(process.argv) }
}

//
// Json Streaming Helper
//

function fetchJsonStream(url, body, onData) {
    const {hostname, port, pathname} = URL.parse(url)
    let buffer = ''
    const onEachData = (chunk, reject) => {
        buffer += chunk

        while (true) {
            const newLineIndex = buffer.indexOf('\n');
            if (newLineIndex === -1) break;

            const line = buffer.slice(0, newLineIndex);
            buffer = buffer.slice(newLineIndex + 1);

            if (!line.trim()) continue;

            try {
                const obj = JSON.parse(line);
                if(obj === null) continue;
                onData(obj);
            } catch (error) {
                reject(new Error(`Invalid JSON in line: ${error.message}`))
            }
        }
    }

    const postBody = JSON.stringify(body) + '\n'

    const options = {
        hostname, port, path: pathname,
        method: 'POST',
        headers: {
            'Content-Type': 'application/json'
        }
    }

    return new Promise((resolve, reject) => {

        const request = http.request(options, resp => {
            if (resp.statusCode !== 200) {
                let errMsg = `[[ HTTP STATUS ${resp.statusCode}, RESPONSE `
                new Promise((r) => {
                    resp.on('data', c => errMsg+= c); resp.on('end', r);
                }).then(() => reject(`${errMsg} ]]`))
                return
            }

            resp.on('data', chunk => onEachData(chunk, reject));
            resp.on('end', () => {
                if (buffer) reject(new Error("Incomplete JSON received"));
                else resolve()
            });
        })
        request.on('error', reject)

        request.write(postBody, (error) => {
            if (error) reject(error);
        })
        request.end()
    })
}

async function createMemory(config) {
    const { memoryFile } = config
    if(!memoryFile) return null

    let memory = {}
    try {
        msg("[[ Chat memory: '" + memoryFile + "' ]]\n")
        await fs.access(memoryFile)
        const content = await fs.readFile(memoryFile, 'utf8')
        const jsonContent = JSON.parse(content)
        if(typeof jsonContent === 'object' && jsonContent !== null) memory = jsonContent
    } catch (e) {
        // Do nothing
    }
    if(!memory.messages) memory.messages = []

    async function saveAsync() {
        // Push the assistant response
        if(this.responseBuffer) this.messages.push({ "role" : "assistant", content: this.responseBuffer })

        const memory = { messages: this.messages }
        await fs.writeFile(memoryFile, JSON.stringify(memory))
        msg("[[ Chat history saved: '" + memoryFile + "' ]]\n")
    }

    function appendAssistantOut(str) {
        if(!this.responseBuffer) this.responseBuffer = ""
        this.responseBuffer += str
    }

    function toolResponse(content) {
        this.messages.push({"role":"tool_call", content})
    }

    function userResponse(content) {
        this.messages.push({"role":"user", content})
    }

    return { messages: memory.messages || [], responseBuffer: null , appendAssistantOut, toolResponse, userResponse, saveAsync }
}

//
// Post Hooks
//
const postHooks = [];
function postHook(a) { postHooks.push(a) }
async function runPostHooks() { await Promise.all(postHooks.map((a) => a())) }

//
// Main
//

;(async () => {
    const config = buildConfig()
    const { model } = config
    if(!model) (msg("Please set the LLM model name.\n\n"),printHelp(),process.exit(5));

    //
    // If has output file, open and prepare the writer
    //
    let dataWriter = await (async () => {
        const outFilePath = config.out
        if(outFilePath) {
            const file = await fs.open(outFilePath, 'w')
            postHook(async () => {
                await file.sync();
                await file.close();
                msg("[[ LLM Response saved to file '" + outFilePath + "' ]]\n")
            })
            return ((line) => file.write(`${line}\n`))
        }
        return () => { /* Noop */ }
    })()

    msg(`Server : ${config.serverUrl}\n`)
    msg(`Model  : ${model}\n`)
    msg(`\n`)

    const request = { model: config.model }

    // Set the keep-alive if needed
    if(config.modelKeepAlive) request.keep_alive = config.modelKeepAlive

    // Create memory (if needed)
    const chatMemory = await createMemory(config)

    // Prepare message
    const messages = []
    if(config.systemPrompt) {
        const systemMessage = await fs.readFile(config.systemPrompt, { encoding: 'utf8' })
        msg(`<< System Role (${config.systemRole}) >>\n`)
        msg(systemMessage)
        msg("\n=================\n\n")
        messages.push({ role: config.systemRole, content: systemMessage })
    }
    // If has memory, restore the memory
    if(chatMemory && (chatMemory.messages.length > 0)) {
        for(const e of chatMemory.messages) messages.push(e);
        msg(`[[ Restored ${chatMemory.messages.length} chat histories. ]]\n`)
    }

    let { prompt } = config
    if (!prompt) {
        prompt = ''
        process.stdin.on('data', chunk => prompt += chunk)
        await new Promise((resolve) => process.stdin.on('end', resolve))
    }
    const userMessage = { role: "user", content: prompt }
    messages.push(userMessage)
    if(chatMemory) chatMemory.userResponse(userMessage.content)

    if (messages.length <= 0) {
        msg("Neither user prompt or system prompt is provided.")
        process.exit(1)
    }

    request.messages = messages
    msg(`<< Question : `)
    msg(`${prompt}\n`)
    msg(`>> Answer   : `)

    // Write the request body to file
    await dataWriter(JSON.stringify(request) + "\n")

    // Request to server
    await fetchJsonStream(`${config.serverUrl}/api/chat`, request, (data) => {
        if (!data) return

        dataWriter(JSON.stringify(data))
            .then(() => {})
            .catch((e) => msg("Fail to write line: " + e.message))

        const {message, done} = data;

        if (done) return msg("\n\n")
        const { content } = message
        print(content)
        chatMemory && chatMemory.appendAssistantOut(content)
    })

    postHook(() => chatMemory.saveAsync())

    // Trigger post hooks
    await Promise.all(postHooks.map((a) => a()))
})().then(() => {}).catch(console.error);
